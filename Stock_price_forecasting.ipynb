{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock price forecasting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stock price forecasting\n"
      ],
      "metadata": {
        "id": "OQTSiQ5RBqyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem description\n",
        "\n",
        "In this challenge, we desire to predict the short-term evolution of the stock market prices of five major companies. According to [Wikipedia](https://en.wikipedia.org/wiki/Stock_market_prediction), **stock market prediction** is defined as:\n",
        "\n",
        "> [...] the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit.\n",
        "\n",
        "Needless to say, the potential returns for a successful approach in this direction is of great interest for financial analysts and traders. However, there is an on-going mathematical and philosophical debate on [whether it is even possible to predict anything about the future evolution of stocks](https://en.m.wikipedia.org/wiki/Random_walk_hypothesis), beyond pure guessing:\n",
        "\n",
        "> The efficient-market hypothesis suggests that stock prices reflect all currently available information and any price changes that are not based on newly revealed information thus are inherently unpredictable. Others disagree and those with this viewpoint possess myriad methods and technologies which purportedly allow them to gain future price information.\n",
        "\n",
        "While the truth might be somewhere in the middle, we'd like to do our best to analyse and predict future evolutions of the stock market."
      ],
      "metadata": {
        "id": "WZSsXUqeIUsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "In preparing our submission, we've referred to the following materials:\n",
        "\n",
        "- Wikipedia articles on [stock market prediction](https://en.wikipedia.org/wiki/Stock_market_prediction), the [efficient-market hyptothesis](https://en.wikipedia.org/wiki/Efficient-market_hypothesis)\n",
        "\n",
        "- TensorFlow's [Time series forecasting tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\n",
        "\n",
        "- Neptune's [Predicting Stock Prices Using Machine Learning](https://neptune.ai/blog/predicting-stock-prices-using-machine-learning)\n",
        "\n",
        "- Abagen's [Data normalization options](https://abagen.readthedocs.io/en/stable/user_guide/normalization.html)\n",
        "\n",
        "- [Stock Closing Price Prediction using Machine Learning Techniques](https://www.sciencedirect.com/science/article/pii/S1877050920307924) by M. Vijha, D. Chandolab, V. A. Tikkiwalb and A. Kumarc\n",
        "\n",
        "- [Stock Price Forecasting by a Deep Convolutional Generative Adversarial Network](https://www.frontiersin.org/articles/10.3389/frai.2022.837596/full) by A. Staffini\n",
        "\n",
        "- [Impact of Data Normalization on Stock Index Forecasting](https://www.researchgate.net/publication/291962265_Impact_of_Data_Normalization_on_Stock_Index_Forecasting) by S. C. Nayak\n",
        "\n",
        "- Pandas's [Market Calendars](https://pandas-market-calendars.readthedocs.io/en/latest/usage.html), Kiplinger's [Stock Exchange Holidays](https://www.kiplinger.com/investing/603728/stock-market-holidays-in-2022) and TDS's [Holidays Calendars Article](https://towardsdatascience.com/holiday-calendars-with-pandas-9c01f1ee5fee)\n",
        "\n",
        "- Medium's ([1](https://medium.datadriveninvestor.com/step-by-step-time-series-analysis-d2f117554d7e)), TDS's ([1](https://towardsdatascience.com/how-to-remove-non-stationarity-in-time-series-forecasting-563c05c4bfc7),[2](https://towardsdatascience.com/stationarity-assumption-in-time-series-data-67ec93d0f2f)) and MLM's ([1](https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/#:~:text=Time%20series%20are%20stationary%20if,the%20variance%20of%20the%20observations.),[2](https://machinelearningmastery.com/difference-time-series-dataset-python/)) Stationarity Time Series Articles"
      ],
      "metadata": {
        "id": "aXNPAL4yIerd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further improvements\n",
        "\n",
        "The following is a non-exhaustive list of ideas on how this project could be expanded and improved.\n",
        "\n",
        "- Define and use _all_ of the extra predictive variables which were introduced in [[1](https://www.sciencedirect.com/science/article/pii/S1877050920307924)] and [[2](https://www.frontiersin.org/articles/10.3389/frai.2022.837596/full)].\n",
        "\n",
        "- Detect price [support and resistance](https://www.investopedia.com/trading/support-and-resistance-basics/) levels using code, based on the approach described [here](https://towardsdatascience.com/detection-of-price-support-and-resistance-levels-in-python-baedc44c34c9).\n",
        "\n",
        "- Add a new price rolling mean, but weighted by **the volume of transactions** for each previous day.\n",
        "\n",
        "- Define new input variables using [exponentially weighted rolling means](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html) (natively supported by `pandas`)\n",
        "\n",
        "- Define a new variable using [Spencer's 15-point weighted moving average](https://mathworld.wolfram.com/Spencers15-PointMovingAverage.html), which is sometimes used by actuaries.\n",
        "\n",
        "- Implement [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) as an error function.\n",
        "\n",
        "- Fine-tune the [XGBoost](https://en.wikipedia.org/wiki/XGBoost) model to run faster and produce better results. The `XGBRegressor` class has quite a few tunable hyperparameters, which are presented in [the XGBoost documentation](https://xgboost.readthedocs.io/en/stable/index.html).\n",
        "\n",
        "- Use deep learning algorithms for stock closing price prediction:\n",
        "  - The TensorFlow tutorial on [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series) provides code for the following models:\n",
        "    - Basic neural network\n",
        "    - Convolutional neural network\n",
        "    - [Long short-term memory](https://www.simplilearn.com/tutorials/machine-learning-tutorial/stock-price-prediction-using-machine-learning)\n",
        "  - The GAN-based method from [this research paper](https://www.frontiersin.org/articles/10.3389/frai.2022.837596/full)"
      ],
      "metadata": {
        "id": "-JS5X_wblNUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n",
        "\n",
        "In this section we obtain a path to the given data file and load it into memory."
      ],
      "metadata": {
        "id": "t9ywrpppQLdb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJbg4idDAH54"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# filename = 'DataSet_Target Portfolio.xlsx'\n",
        "filename = 'DataSet_Target Portfolio - exported on 19th of April.xlsx'\n",
        "\n",
        "# Check if we're running on Colab or not\n",
        "running_on_google_colab = 'google.colab' in str(get_ipython())\n",
        "if running_on_google_colab:\n",
        "    from google.colab import files\n",
        "\n",
        "    dataset_file = Path(filename)\n",
        "    if not dataset_file.is_file():\n",
        "        files.upload()\n",
        "else:\n",
        "    dataset_file = Path('/mnt/d/Libraries/Downloads/') / Path(filename)\n",
        "\n",
        "assert dataset_file.is_file(), 'Could not open dataset file!'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5m56AGm6Hy3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ('date', 'open', 'high', 'low', 'close', 'adj_close', 'volume')\n",
        "index_column_name = column_names[0]\n",
        "sheet_names = ('GS', 'C', 'WFC', 'BAC', 'JPM')\n",
        "\n",
        "datasets = {}\n",
        "for sheet_name in sheet_names:\n",
        "    dataset = pd.read_excel(dataset_file,\n",
        "                            names=column_names,\n",
        "                            index_col=index_column_name,\n",
        "                            sheet_name=sheet_name)\n",
        "    datasets[sheet_name] = dataset\n",
        "\n",
        "print('Successfully read stock data for', len(datasets), 'companies:')\n",
        "print(' ', ', '.join(datasets.keys()))"
      ],
      "metadata": {
        "id": "osBPN72aJMvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since making all the plots for the companies in the input data in the same notebook would be tiresome, we're going to pick one of them and store it in the `dataset` variable, which we will reuse throughout the notebook:"
      ],
      "metadata": {
        "id": "fAyeSOwkj_Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets['GS']\n",
        "dataset"
      ],
      "metadata": {
        "id": "NJgiQlzGjHGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory data analysis"
      ],
      "metadata": {
        "id": "P7-Q5g_fQSMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary statisics"
      ],
      "metadata": {
        "id": "VQv285SbQNQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 5)\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "metadata": {
        "id": "3Dyd7irZJwe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe().transpose()"
      ],
      "metadata": {
        "id": "AVlM-eSxOA_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "1y7gEq1RJidR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.open.hist()\n",
        "plt.title(\"Open values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AWufLEB8QUnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting a sense of the series:"
      ],
      "metadata": {
        "id": "ZmVFX1AGEAIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = dataset.iloc[0:]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(subset.index, subset.open, label = 'open')\n",
        "ax.plot(subset.index, subset.high, label = 'high')\n",
        "ax.plot(subset.index, subset.low, label = 'low')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2YHaPSdpKepL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dataset.index, np.abs(dataset.close - dataset.adj_close))\n",
        "plt.title(\"The difference between Close and Adjusted Close\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tQE_MmIJNIao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.suptitle('Volume of transactions over time')\n",
        "\n",
        "plt.plot(dataset.index, dataset.volume)\n",
        "\n",
        "plt.ylabel('Volume (units)')\n",
        "plt.xlabel('Date')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gbV23Y64LUCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing data"
      ],
      "metadata": {
        "id": "MlFm0pW4jkTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look for missing values in the data:"
      ],
      "metadata": {
        "id": "w9xXEaXq4P6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "HveiwNKWOHYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate all the bussiness days between the first and last entry to check if there are missing days:"
      ],
      "metadata": {
        "id": "JxdASlYp4Ukn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = dataset.head(1).index[0]\n",
        "end_date = dataset.tail(1).index[0]\n",
        "\n",
        "dates = pd.bdate_range(start=start_date, end=end_date)\n",
        "\n",
        "print(\"The difference in the number of days: \" + str(abs(dates.shape[0] - dataset.shape[0])))"
      ],
      "metadata": {
        "id": "C4Oq6zqb083K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We considered only the weekdays for the range. Let's get all the holidays out of the working range:"
      ],
      "metadata": {
        "id": "ico8cPBR5CZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas-market-calendars"
      ],
      "metadata": {
        "id": "v5HBGzSqCrnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_market_calendars as mcal\n",
        "\n",
        "nyse = mcal.get_calendar('NYSE')\n",
        "dates = nyse.valid_days(start_date=start_date, end_date=end_date)\n",
        "\n",
        "holidays_missmatch = False\n",
        "for date in range(dates.shape[0]):\n",
        "  if str(dataset.index[date])[:10] != str(dates[date])[:10]:\n",
        "    holidays_missmatch = True\n",
        "    break\n",
        "\n",
        "if holidays_missmatch:\n",
        "  print(\"There is a mismatch between the dataset and holidays dataset!\")\n",
        "else:\n",
        "  print(\"The dataset matches the holidays dataset!\")\n"
      ],
      "metadata": {
        "id": "vZY2Y-Tg-nHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the NYSE holidays for the matching. However, if we want to check for a different series of holidays, we can manually insert them as so:"
      ],
      "metadata": {
        "id": "QqRjiZqfCpVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.tseries.holiday import *\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "\n",
        "class HolidaysCalendar(AbstractHolidayCalendar):\n",
        "    rules = [\n",
        "        Holiday('New Year', month = 1, day = 1, observance = sunday_to_monday),\n",
        "        Holiday('Groundhog Day', month = 1, day = 6, observance = sunday_to_monday),\n",
        "        Holiday('St. Patricks Day', month = 3, day = 17, observance = sunday_to_monday),\n",
        "        Holiday('April Fools Day', month = 4, day = 1),\n",
        "        Holiday('Good Friday', month = 1, day = 1, offset = [Easter(), Day(-2)]),\n",
        "        Holiday('Labor Day', month = 5, day = 1, observance = sunday_to_monday),\n",
        "        Holiday('Canada Day', month = 7, day = 1, observance = sunday_to_monday),\n",
        "        Holiday('July 4th', month = 7, day = 4, observance = nearest_workday),\n",
        "        Holiday('All Saints Day', month = 11, day = 1, observance = sunday_to_monday),\n",
        "        Holiday('Christmas', month = 12, day = 25, observance = nearest_workday)\n",
        "    ]\n",
        "\n",
        "_holidays = CustomBusinessDay(calendar = HolidaysCalendar())\n",
        "_dates = pd.bdate_range(start = start_date, end = end_date, freq = _holidays)\n",
        "_dates = pd.DataFrame(_dates, columns = ['date'])\n",
        "\n",
        "df = dataset.index - _dates['date']\n",
        "# print(\"Are there any mismatches? \" + str(df.any()))"
      ],
      "metadata": {
        "id": "1mj2Udtp3xJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for outliers\n",
        "\n",
        "We will check for outliers using the [ThymeBoost](https://towardsdatascience.com/time-series-outlier-detection-with-thymeboost-ec2046e17458) library. ThymeBoost is a toolkit for time series decomposition using gradient boosting tehniques, but we'll use it specifically for extracting valuable information about stationarity, seasonality, etc."
      ],
      "metadata": {
        "id": "wWnRiHL0vx5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first have to install this library, if it's not already present:"
      ],
      "metadata": {
        "id": "YMLMjsdwztf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet ThymeBoost"
      ],
      "metadata": {
        "id": "Nt-ZrB3MDIj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a new boosted model, and use it to detect outliers in the stock's closing prices:"
      ],
      "metadata": {
        "id": "rXMTGk35zynJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ThymeBoost import ThymeBoost as tb\n",
        "\n",
        "boosted_model = tb.ThymeBoost()\n",
        "output = boosted_model.detect_outliers(dataset.close.values,\n",
        "                                       trend_estimator='linear',\n",
        "                                       seasonal_estimator='fourier',\n",
        "                                       seasonal_period=25,\n",
        "                                       global_cost='maicc',\n",
        "                                       fit_type='global')"
      ],
      "metadata": {
        "id": "6_j8AlxCv0zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boosted_model.plot_results(output)\n",
        "boosted_model.plot_components(output)"
      ],
      "metadata": {
        "id": "Wk9vXEptzqKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dy = output.trend[1] - output.trend[0]\n",
        "dx = 1 - 0\n",
        "slope = dy/dx\n",
        "\n",
        "print('slope:', slope)"
      ],
      "metadata": {
        "id": "WnmVrdF1z8on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots created by ThymeBoost are fairly informative:\n",
        "\n",
        "- The stock market has an overall ascending trend, with a slope of about $0.07$.\n",
        "\n",
        "- Besides the smaller usual variations of the closing market price, we see that the biggest outliers found by ThymeBoost are in in the first quarters of 2020 and 2022. The first one has been caused by the [CoVID pandemic](https://en.wikipedia.org/wiki/List_of_stock_market_crashes_and_bear_markets) (2020), while the second one is an inflationary wave (caused by the post-pandemic economic recovery effort), ended by the Russian invasion of Ukraine.\n",
        "\n",
        "- There doesn't seem to be any discernible seasonality in the data; seasonal effects look like random noise.\n",
        "\n",
        "We can also try to identify different local patterns in the charts (e.g. the head and shoulders pattern) and correlate them with real-life economic events."
      ],
      "metadata": {
        "id": "L4xr5B8u1wQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying to detect seasonality in the data\n",
        "\n",
        "Predicting the stock market is very different from predicting the weather; there is no discernible pattern in the data, since the growth of the economy doesn't really depend on the season or the day of the week. That being said, we can use the [Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform) to convince ourselves that there really isn't any sort of recurring behavior, no matter the level of the scale we're looking at:"
      ],
      "metadata": {
        "id": "1msXmzpJr789"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "closing_price_frequency_spectrum = np.fft.rfft(dataset.close)\n",
        "\n",
        "closing_price_spectrum_abs = np.abs(closing_price_frequency_spectrum)\n",
        "closing_price_spectrum_abs /= np.max(closing_price_spectrum_abs)\n",
        "\n",
        "plt.plot(closing_price_spectrum_abs)\n",
        "\n",
        "plt.ylabel('Closing price value (normalized)')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Frequency (log scale)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gtgckrcBsBx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closing_price_deltas = np.abs(np.diff(dataset.close))\n",
        "closing_price_deltas_frequency_spectrum = np.fft.rfft(closing_price_deltas)\n",
        "\n",
        "closing_price_deltas_abs = np.abs(closing_price_deltas_frequency_spectrum)\n",
        "closing_price_deltas_abs /= np.max(closing_price_deltas_abs)\n",
        "\n",
        "plt.plot(closing_price_deltas_abs)\n",
        "\n",
        "plt.ylabel('Closing price delta (normalized)')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Frequency (log scale)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zSRqBdvTTFqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volume_frequency_spectrum = np.fft.rfft(dataset.volume)\n",
        "\n",
        "volume_spectrum_abs = np.abs(volume_frequency_spectrum)\n",
        "volume_spectrum_abs /= np.max(volume_spectrum_abs)\n",
        "\n",
        "plt.plot(volume_spectrum_abs)\n",
        "\n",
        "plt.ylabel('Volume (normalized)')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Frequency (log scale)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "22H00Zb3PcXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The stationarity of the data\n",
        "\n",
        "[Stationary data](https://towardsdatascience.com/stationarity-assumption-in-time-series-data-67ec93d0f2f) refers to the time series data that mean and variance do not vary across time. The data is considered non-stationary if there is a strong trend or seasonality observed from the data.\n",
        "\n",
        "Using non-stationary time series data in financial models produces unreliable and spurious results and leads to poor understanding and [forecasting](https://www.researchgate.net/post/Is_the_stock_return_series_ALWAYS_stationary). The solution to the problem is to transform the time series data so that it becomes stationary.\n",
        "\n",
        "We will use the Augmented Dickey-Fuller test to check for the stationarity of our data. We want a p-value lower than $0.05$."
      ],
      "metadata": {
        "id": "CIwYNUysFDiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "test_results = adfuller(dataset[\"close\"])\n",
        "\n",
        "print(f\"ADF test statistic: {test_results[0]}\")\n",
        "print(f\"p-value: {test_results[1]}\")\n",
        "print(\"Critical thresholds:\")\n",
        "\n",
        "for key, value in test_results[4].items():\n",
        "    print(f\"\\t{key}: {value}\")"
      ],
      "metadata": {
        "id": "T7SF9WOqF6uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the ADF statistical test we have the following:\n",
        "- *The null hypothesis*: the distribution is non-stationary, time-dependent (it has a unit root).\n",
        "- *The alternative hypothesis*: the distribution is stationary, not time-dependent (canâ€™t be represented by a unit root).\n",
        "\n",
        "The p-value determines the result of the test. If it is smaller than a critical threshold of 0.05 or 0.01, we reject the null hypothesis and conclude that the series is stationary. Otherwise, we fail to reject the null and conclude the series is non-stationary.\n",
        "\n",
        "As expected, we have a high p-value. Let's try to check the differencing of the time series:"
      ],
      "metadata": {
        "id": "tToI_HqbJn2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[\"close\"].copy()\n",
        "\n",
        "plt.plot(data)\n",
        "plt.title(\"The time series\")\n",
        "plt.show()\n",
        "\n",
        "data_difference = data.diff()\n",
        "plt.plot(data_difference)\n",
        "plt.title(\"The first difference time series\")\n",
        "plt.show()\n",
        "\n",
        "data_difference_second = data_difference.diff()\n",
        "plt.plot(data_difference_second)\n",
        "plt.title(\"The second difference time series\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pQQDC2VmG2o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like a single differencing process is enough. Let's repeat the ADF test for the once differenced time series:"
      ],
      "metadata": {
        "id": "kJp1ir9zJvhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get rid of the first value (NaN)\n",
        "data_difference[0] = data_difference[1]\n",
        "\n",
        "test_results = adfuller(data_difference)\n",
        "\n",
        "print(f\"ADF test statistic: {test_results[0]}\")\n",
        "print(f\"p-value: {test_results[1]}\")\n",
        "print(\"Critical thresholds:\")\n",
        "\n",
        "for key, value in test_results[4].items():\n",
        "    print(f\"\\t{key}: {value}\")"
      ],
      "metadata": {
        "id": "mXmPBaXcJ3sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our p-value is almost 0. This means we can easily reject the null hypothesis and consider the distribution as stationary.\n",
        "\n",
        "The difference will be done manually, so we can reverse the changes at the end."
      ],
      "metadata": {
        "id": "PVZ0PoaAKqKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def difference(dataset, interval = 1):\n",
        "\tdiff = list()\n",
        "\tfor i in range(interval, len(dataset)):\n",
        "\t\tvalue = dataset[i] - dataset[i - interval]\n",
        "\t\tdiff.append(value)\n",
        "\tdiff.insert(0, np.nan)\n",
        "\treturn diff\n",
        " \n",
        "def inverse_difference(last_ob, value):\n",
        "\treturn value + last_ob\n",
        " \n",
        "data = data.values\n",
        "plt.plot(data)\n",
        "plt.title(\"The time series\")\n",
        "plt.show()\n",
        "\n",
        "diff = difference(data)\n",
        "plt.plot(diff)\n",
        "plt.title(\"The difference time series\")\n",
        "plt.show()\n",
        "\n",
        "inverted = [inverse_difference(data[i], diff[i]) for i in range(len(diff))]\n",
        "inverted.insert(0, data[0])\n",
        "plt.plot(inverted)\n",
        "plt.title(\"The inverted difference time series\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pipZBwoxPKKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = diff.pop(0)"
      ],
      "metadata": {
        "id": "YU13jxS9sNwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the *ThymeBoost* statistics on the differenced time series:"
      ],
      "metadata": {
        "id": "qpZF2Tr3xshs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boosted_model = tb.ThymeBoost()\n",
        "output = boosted_model.detect_outliers(diff,\n",
        "                                       trend_estimator = 'linear',\n",
        "                                       seasonal_estimator = 'fourier',\n",
        "                                       seasonal_period = 25,\n",
        "                                       global_cost = 'maicc',\n",
        "                                       fit_type = 'global')\n",
        "boosted_model.plot_results(output)\n",
        "boosted_model.plot_components(output)"
      ],
      "metadata": {
        "id": "YdXPvhn9xWHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data range"
      ],
      "metadata": {
        "id": "2kbnp41c5_p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like we need to normalize the data, which we'll do anyway before putting it into the models."
      ],
      "metadata": {
        "id": "kLAAEpHox1W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.volume.hist()\n",
        "plt.title(\"Volume values\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "H1yeISoTQair"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n"
      ],
      "metadata": {
        "id": "6JWaTiKfVifA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional variables\n",
        "\n",
        "Besides the raw timeseries data, traders who perform technical analysis on stock prices also extract from the data some additional aggregate statistics, (potentially weighted) moving averages and other derived indicators. We'll do so as well, with the intention of providing additional information to the  statistical models.\n",
        "\n",
        "The additional variables we define in these two papers [[1](https://www.sciencedirect.com/science/article/pii/S1877050920307924), [2](https://www.frontiersin.org/articles/10.3389/frai.2022.837596/full)] . We did not do ablation tests to verify if each of these additional variables helps improve the models' predicitive power, instead expecting the models to assign lesser weights to less relevant input columns."
      ],
      "metadata": {
        "id": "2DGLHG7K5XBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['high_low_difference'] = dataset['high'] - dataset['low']\n",
        "dataset['close_open_difference'] = dataset['close'] - dataset['open']\n",
        "dataset['seven_days_moving_average'] = dataset['close'].rolling(7, closed='both').mean().fillna(dataset['close'].iloc[0])\n",
        "dataset['fourteen_days_moving_average'] = dataset['close'].rolling(14, closed='both').mean().fillna(dataset['close'].iloc[0])\n",
        "dataset['twenty_one_days_moving_average'] = dataset['close'].rolling(21, closed='both').mean().fillna(dataset['close'].iloc[0])\n",
        "dataset['seven_days_stddev_moving_average'] = dataset['close'].rolling(7, closed='both').std().fillna(1)\n",
        "\n",
        "dataset['close_diff'] = difference(dataset['close'])"
      ],
      "metadata": {
        "id": "mTVxkiFVVjpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at how these new variables look like:"
      ],
      "metadata": {
        "id": "bC9VXH_NjwlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = dataset[-500:]\n",
        "\n",
        "plt.plot(subset.index, subset['close'], label='closing price')\n",
        "plt.plot(subset.index, subset['seven_days_moving_average'], label='7-day moving average')\n",
        "plt.plot(subset.index, subset['fourteen_days_moving_average'], label='14-day moving average')\n",
        "plt.plot(subset.index, subset['twenty_one_days_moving_average'], label='21-day moving average')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pRc554dg0Y85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define some constants for our variables available for making our prediction:"
      ],
      "metadata": {
        "id": "gy-ewsjIFsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AVAILABLE_VARIABLES = [\n",
        "    'open', 'close', 'adj_close', 'high', 'low', 'volume',\n",
        "    'high_low_difference', 'close_open_difference',\n",
        "    'seven_days_moving_average',\n",
        "    'fourteen_days_moving_average',\n",
        "    'twenty_one_days_moving_average',\n",
        "    'seven_days_stddev_moving_average',\n",
        "    'close_diff',\n",
        "]"
      ],
      "metadata": {
        "id": "vSqEmPqTFlfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differencing"
      ],
      "metadata": {
        "id": "gHJpFYbW2mn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_differences(arr):\n",
        "    diffs = np.diff(arr)\n",
        "    return np.insert(diffs, 0, 0)\n",
        "\n",
        "def reverse_differences(diffs, offset):\n",
        "    diffs = np.delete(diffs, 0)\n",
        "    return np.cumsum(np.insert(diffs, 0, offset))"
      ],
      "metadata": {
        "id": "2oAQpQ4X30RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "differences = compute_differences(dataset.close)"
      ],
      "metadata": {
        "id": "AUERpdRn4Epe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure, (ax1, ax2) = plt.subplots(2, 1)\n",
        "figure.suptitle('Histogram of closing prices vs. closing price deltas')\n",
        "\n",
        "sns.histplot(dataset.close, stat='probability', ax=ax1)\n",
        "sns.histplot(differences, kde=True, stat='probability', ax=ax2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-MTUsjrZkmhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverted = reverse_differences(differences, dataset.close[0])"
      ],
      "metadata": {
        "id": "Z6uvAOhGklxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(len(dataset)), dataset.close)\n",
        "plt.plot(np.arange(len(dataset)), reverted)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lwUUC6b53FLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.abs(reverted - dataset.close).max()"
      ],
      "metadata": {
        "id": "GdYQAhky4ylT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.close = compute_differences(dataset.close)"
      ],
      "metadata": {
        "id": "PIHa9OcG46UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the validation set"
      ],
      "metadata": {
        "id": "_8PNZokJeQA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training any models, we'll first split our given data into two parts: the first 2100 samples will be used for training the model to predict the future, and the last ~400 samples will be novel, unseen data on which we will evaluate the models' performance."
      ],
      "metadata": {
        "id": "kPi6cYfoeo91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = dataset.iloc[:2100].copy()\n",
        "validation_set = dataset.iloc[2100:].copy()"
      ],
      "metadata": {
        "id": "zvv_F4RoeTP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the split leads to the validation dataset starting in August 2020, after CoVID hit and the financial markets started adapting to the new reality:"
      ],
      "metadata": {
        "id": "3d0snlsVenWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set_first_timestamp = validation_set.index[0]\n",
        "\n",
        "print(validation_set_first_timestamp.strftime('%Y-%m-%d'))"
      ],
      "metadata": {
        "id": "UWQNPDMieoPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data normalization"
      ],
      "metadata": {
        "id": "ieSAyeIzvIXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make model training more efficient and to improve its predictive performance, data is usually **normalized** before being fed into a machine learning algorithm. This is done to ensure that the (absolute) variations in the data points are at about the same scale, improving the numerical stability of the machine learning methods we'll use.\n",
        "\n",
        "All of these normalization functions are reversible."
      ],
      "metadata": {
        "id": "zqv9e3-RmjTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining which kind of normalization to use"
      ],
      "metadata": {
        "id": "BeU1sGSCmjQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Min-Max Normalization:** Normalizes the data, converting the closed inverval $[\\min, \\max]$ into $[0, 1]$. The main problem with this method is that we could have values outside of our interval in the future. However, given a dataset that spans a long time interval, the assumption that new values do not lie outside our interval is usually accepted.\n",
        "\n",
        "$$ \\hat{x} = \\frac{x - min}{max - min} $$"
      ],
      "metadata": {
        "id": "hdtlu3t7Jmn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_min_max(training_set, validation_set):\n",
        "\n",
        "  min = training_set.min()\n",
        "  max = training_set.max()\n",
        "\n",
        "  training_set = (training_set - min) / (max - min)\n",
        "  validation_set = (validation_set - min) / (max - min)\n",
        "\n",
        "  return training_set, validation_set"
      ],
      "metadata": {
        "id": "qbps5_KpFrHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Z-Score Normalization:** Normalizes the data, converting the mean to 0 and the standard deviation to 1. Best used when the data is normally distributed and the time series is stationary.\n",
        "\n",
        "$$ \\hat{x} = \\frac{x - \\mu}{\\sigma} $$"
      ],
      "metadata": {
        "id": "u7Rv4YZRKoEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_mean_std(training_set, validation_set):\n",
        "\n",
        "  mean = training_set.mean()\n",
        "  std = training_set.std()\n",
        "\n",
        "  training_set = (training_set - mean) / std\n",
        "  validation_set = (validation_set - mean) / std\n",
        "\n",
        "  return training_set, validation_set"
      ],
      "metadata": {
        "id": "roTMAPw_vJ_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Median Value Normalization:** Normalizes the data by dividing each sample by the median. The median is not influenced by the magnitude of extreme deviations, so it can handle outliers with ease.\n",
        "\n",
        "$$ \\hat{x} = \\frac{x}{\\text{median}(x)} $$"
      ],
      "metadata": {
        "id": "GDor4PpCLL8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_mean(training_set, validation_set):\n",
        "\n",
        "  median = training_set.median()\n",
        "\n",
        "  training_set = training_set / median\n",
        "  validation_set = validation_set / median\n",
        "\n",
        "  return training_set, validation_set"
      ],
      "metadata": {
        "id": "SEm-Mm_CLFXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid Normalization:** Normalizes the data by replacing each sample $s$ with $(1-e^s)^{-1}$. The working interval becomes $[0,1]$ and this normalization method is great for when we do not know the underlying distribution of our data. We used the a standard sigmoid here, but it is worth mentioning that there are other similar ways to do this task (i.e. scaled sigmoid, robust sigmoid, etc.).\n",
        "\n",
        "$$ \\hat{x} = \\frac{1}{1 - e^{x}} $$"
      ],
      "metadata": {
        "id": "egG0wxiILxS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_sigmoid(training_set, validation_set):\n",
        "\n",
        "  training_set = 1 / (1 - np.exp(1) ** training_set)\n",
        "  validation_set = 1 / (1 - np.exp(1) ** validation_set) \n",
        "\n",
        "  return training_set, validation_set"
      ],
      "metadata": {
        "id": "bmZNtWL1Mpb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple other ways to normalize the data (e.g. arctangent estimators, median and median absolute deviation, etc.). However, for the given task, it seems that the optimal choice is the **Min-Max Normalization**, so this will be the method we will use in building the models. The implementation used will be the one provided by *sklearn*, as it already implements all the needed functionality."
      ],
      "metadata": {
        "id": "6JEAYK9dNg-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "values = pd.DataFrame(training_set['close'].values)\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "scaler = scaler.fit(values)\n",
        "\n",
        "print('Min: %f\\nMax: %f' % (scaler.data_min_, scaler.data_max_))"
      ],
      "metadata": {
        "id": "1KelKsanUnE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set"
      ],
      "metadata": {
        "id": "8E0DQ4EmSeVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the values and check the result:"
      ],
      "metadata": {
        "id": "gyN5GysxW5Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_values = scaler.transform(values)\n",
        "\n",
        "normalized_values"
      ],
      "metadata": {
        "id": "uX8qeA0wWq63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restore the normalized values and check the result:"
      ],
      "metadata": {
        "id": "kKDf_UK6W-E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "restored_values = scaler.inverse_transform(normalized_values)\n",
        "\n",
        "restored_values"
      ],
      "metadata": {
        "id": "I_dDIzSNWsvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the dataset"
      ],
      "metadata": {
        "id": "C1xsF9kt1fUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalers = {}\n",
        "\n",
        "for variable in AVAILABLE_VARIABLES:\n",
        "    scalers[variable] = MinMaxScaler(feature_range=(0, 1))\n",
        "    scalers[variable] = scalers[variable].fit(dataset[variable].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    training_set[variable] = scalers[variable].transform(pd.DataFrame(training_set[variable].values))\n",
        "    validation_set[variable] = scalers[variable].transform(pd.DataFrame(validation_set[variable].values))"
      ],
      "metadata": {
        "id": "gfkwZ3gNJQGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set"
      ],
      "metadata": {
        "id": "SquI2Kl_tvfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting outliers"
      ],
      "metadata": {
        "id": "MX1uahM02ThR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop the NaN's created by the differencing:"
      ],
      "metadata": {
        "id": "sMn2V6OG0_nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = training_set.dropna()\n",
        "validation_set = validation_set.dropna()"
      ],
      "metadata": {
        "id": "Hyg0LOpxtnpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the data\n",
        "# training_set['close'] = scaler.inverse_transform(pd.DataFrame(training_set['close'].values))\n",
        "# validation_set['close'] = scaler.inverse_transform(pd.DataFrame(validation_set['close'].values))"
      ],
      "metadata": {
        "id": "TbLkbv08tYQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boosted_model = tb.ThymeBoost()\n",
        "output = boosted_model.detect_outliers(training_set['close'].values,\n",
        "                                       trend_estimator = 'linear',\n",
        "                                       seasonal_estimator = 'fourier',\n",
        "                                       seasonal_period = 25,\n",
        "                                       global_cost = 'maicc',\n",
        "                                       fit_type = 'global')\n",
        "boosted_model.plot_results(output)\n",
        "boosted_model.plot_components(output)"
      ],
      "metadata": {
        "id": "WqX0o-2Tz_0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a violin plot of the values in the training set and see that they're almost normally distributed around 0:"
      ],
      "metadata": {
        "id": "_mTkDRUA3Krr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = training_set.melt(var_name='Column', value_name='Normalized')\n",
        "ax = sns.violinplot(x='Column', y='Normalized', data=df)\n",
        "ax.set_xticklabels(training_set.keys(), rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CGLy2Obm25SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model construction and evaluation\n",
        "\n",
        "Now that we have a much better understanding of the data we're working with and have set up the scene for further experimentation on it, in this section we will attempt to construct a few **models** to help us predict the future based on patterns we've seen in the past."
      ],
      "metadata": {
        "id": "xwlX0UZJeD6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The libraries we are going to use will be `sklearn` (for simpler, linear regression-based models, as well as random forests) and `xgboost` (for gradient-boosted ensembles of trees)."
      ],
      "metadata": {
        "id": "lWxhsbJwJ9nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import xgboost\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "vBdkpXrhIYNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by storing the name of the variable (column) we want to predict in a Python constant. We're interested in forecasting each future day's closing stock price, since that's a relevant indicator for understanding the evolution of a company's performance over time."
      ],
      "metadata": {
        "id": "4wo4deIwJ9q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_VARIABLE = 'close'"
      ],
      "metadata": {
        "id": "N0vNK92wJ99B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error function"
      ],
      "metadata": {
        "id": "RDRIkPsRvrsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to evaluate the performance of our models, we need to define an **error function**, which will help us compute a numerical value, describing how far off our predictions are from reality.\n",
        "\n",
        "According to Wikipedia, a [forecasting error](https://en.wikipedia.org/wiki/Forecast_error) is\n",
        "\n",
        "> the difference between the actual or real and the predicted or forecast value of a time series [..]\n",
        "\n",
        "We're not going to use the differences between the real and predicted values directly, rather we'll put them through some formulas (described below)."
      ],
      "metadata": {
        "id": "9E5EAuxvzVqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean absolute error\n",
        "\n",
        "One of the simplest error functions to understand is [the mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error). We take the differences between every forecasted value and the real value, compute the absolute value of each difference and sum these up.\n",
        "\n",
        "$$\\mathrm{MAE} = \\frac{1}{n} \\sum_{i = 1}^{n} \\lvert y_i - \\hat{y}_i \\rvert$$ "
      ],
      "metadata": {
        "id": "oYLbcrcfzUDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_error(expected, predicted):\n",
        "    differences = expected - predicted\n",
        "    absolute_differences = np.absolute(differences)\n",
        "    return absolute_differences.mean()"
      ],
      "metadata": {
        "id": "oaDFVkbWsRce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean squared error\n",
        "\n",
        "Another straightforward error function, often used in regression problems, is [the mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error). Just like before, we compute the elementwise differences between the target values and the forecasts, then square them and sum them up.\n",
        "\n",
        "$$\\mathrm{MSE} = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2$$"
      ],
      "metadata": {
        "id": "chERQ59P0Lhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(expected, predicted):\n",
        "    differences = expected - predicted\n",
        "    squared_differences = differences ** 2\n",
        "    return squared_differences.mean()"
      ],
      "metadata": {
        "id": "y2KAfWrc01b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean absolute percentage error\n",
        "\n",
        "An error function which is quite popular in forecasting is [the mean absolute percentage error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error). It's not very difficult to define and has an intuitive interpretation (it's just like the MAE, but offers a percentual/relative error).\n",
        "\n",
        "$$\\mathrm{MAPE} = 100\\% \\cdot \\frac{1}{n} \\sum_{i = 1}^{n} \\left\\lvert\\frac{y_i - \\hat{y}_i}{y_i}\\right\\rvert$$\n",
        "\n",
        "Due to it's formulation, MAPE cannot be used in situations where the predicted value is zero. Fortunately, none of our stock prices ever go down to values close to nil."
      ],
      "metadata": {
        "id": "UYELjIjg1Gnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(expected, predicted):\n",
        "    differences = expected - predicted\n",
        "    relative_absolute_differences = np.absolute(differences / expected)\n",
        "    return relative_absolute_differences.mean() * 100"
      ],
      "metadata": {
        "id": "WZ6BkaOY1IyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean absolute scaled error\n",
        "\n",
        "[The mean absolute scaled error](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error) is similar to the mean absolute error, but addresses some of it's limitations:\n",
        "\n",
        "- it's independent of the scale of the data\n",
        "- it works correctly even when the difference between the predicted value and the real value tends to zero (unlike the MAPE)\n",
        "- it's easier to interpret; if the error is greater than one, it means that the prediction is worse than a \"baseline\" prediction of just assuming that the stock price stayed constant compared to the previous day\n",
        "\n",
        "$$\\mathrm{MASE} = \\frac{\\frac{1}{n} \\sum_{i = 1}^{n} \\lvert y_i - \\hat{y}_i \\rvert}{\\frac{1}{n - 1} \\sum_{j = 2}^{n} \\lvert y_j - y_{j - 1} \\rvert}$$"
      ],
      "metadata": {
        "id": "ktd5mN4IT1eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_scaled_error(expected, predicted):\n",
        "    differences = expected - predicted\n",
        "    absolute_differences = np.absolute(differences)\n",
        "    baseline_differences = np.diff(expected)\n",
        "    return absolute_differences.mean() / baseline_differences.mean()"
      ],
      "metadata": {
        "id": "1Ex7bv89T4YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Putting them all together\n",
        "\n",
        "Since we don't now from the start which error functions will be better predictors of actual model performance, we're also going to define a helper function which will compute all of the pre-defined metrics for a model's output. We'll use these later when comparing the accuracy of the models."
      ],
      "metadata": {
        "id": "mkFpwHTP3qRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ERRORS = ['MAE', 'MSE', 'MAPE', 'MASE']\n",
        "\n",
        "def compute_errors(expected, predicted):\n",
        "    return {\n",
        "        'MAE': mean_absolute_error(expected, predicted),\n",
        "        'MSE': mean_squared_error(expected, predicted),\n",
        "        'MAPE': mean_absolute_percentage_error(expected, predicted),\n",
        "        'MASE': mean_absolute_scaled_error(expected, predicted),\n",
        "    }"
      ],
      "metadata": {
        "id": "lO2nRz393fQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data windowing\n",
        "\n",
        "Considering and the nature of this challenge, and to reflect a realistic use case for the constructed models, we're going to use the following data windowing pattern during model validation:\n",
        "\n",
        "- The models are trained on all available data, up to the last day before the forecasts are made.\n",
        "- The models have to predict the closing price of the respective stocks for the next 5 days (ignoring market holidays).\n",
        "- Afterwards, they're given access to the real data for the 5 predicted days, are refitted and the process is repeated."
      ],
      "metadata": {
        "id": "WWo4ELWh4YT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 5"
      ],
      "metadata": {
        "id": "atStpHpcIodQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model API\n",
        "\n",
        "To make it simpler to construct, train, evaluate and use the models we're going to design, we shall first define an interface for all models to implement:"
      ],
      "metadata": {
        "id": "azPfkRrU5Zen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class StockPriceForecastingModel(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self, data, delta):\n",
        "        '''Trains the model on all of the available data up to\n",
        "        the current validation timestamp.\n",
        "        \n",
        "        - `data` is a pandas `DataFrame` which contains (at the least) the columns\n",
        "        `open`, `close`, `low`, `high` and `volume`.\n",
        "        - `delta` is a pandas `DataFrame` containing only the most recently added data,\n",
        "        compared to the rows of data from the last call. It is `None` on the first call to `train`.\n",
        "        '''\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, timestamps):\n",
        "        '''Requests the model to predict the closing price of the input stock\n",
        "        for five consecutive days.\n",
        "        \n",
        "        The dates are passed in as an array of pandas `Timestamp`s. The model\n",
        "        must return an array of predicted stock prices (real numbers).\n",
        "        '''"
      ],
      "metadata": {
        "id": "9F_QDeL75LAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're also going to extract the common training and evaluation code into a reusable function:"
      ],
      "metadata": {
        "id": "oUj_ynFEG8RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import trange\n",
        "\n",
        "def train_and_evaluate_model(model):\n",
        "    current_training_set = training_set.copy()\n",
        "\n",
        "    # Initial (cold) training of the model on the training set.\n",
        "    model.train(current_training_set, None)\n",
        "\n",
        "    # Record the predictions the model makes on each window, for plotting.\n",
        "    predictions = []\n",
        "\n",
        "    # Record the history of error values as we progress through the validation set.\n",
        "    errors = []\n",
        "\n",
        "    window_num = 1\n",
        "    window_start = 0\n",
        "    window_end = window_start + WINDOW_SIZE\n",
        "\n",
        "    num_windows = math.ceil(len(validation_set) / WINDOW_SIZE)\n",
        "\n",
        "    for window_index in trange(num_windows):\n",
        "        window_start = window_index * WINDOW_SIZE\n",
        "        window_end = (window_index + 1) * WINDOW_SIZE\n",
        "\n",
        "        # Extract the window of data from the validation set\n",
        "        window = validation_set.iloc[window_start:window_end]\n",
        "\n",
        "        # Make some forecasts\n",
        "        predicted = model.predict(window.index)\n",
        "\n",
        "        # Check that the model respect's the `StockPriceForecastingModel` interface.\n",
        "        assert isinstance(predicted, pd.Series), \\\n",
        "            '`predict` must return a pandas `Series`'\n",
        "\n",
        "        assert (window.index == predicted.index).all(), \\\n",
        "            'returned series must have same index as given as input'\n",
        "\n",
        "        predictions.append(predicted)\n",
        "\n",
        "        # Evaluate the results\n",
        "        expected = window.close\n",
        "        window_errors = compute_errors(expected, predicted)\n",
        "        \n",
        "        #print(f'Window #{window_num}:', window_errors)\n",
        "\n",
        "        errors.append(window_errors)\n",
        "\n",
        "        # Now expand the available dataset\n",
        "        current_training_set = pd.concat((current_training_set, window))\n",
        "\n",
        "        # And allow the model to improve it's understanding\n",
        "        model.train(current_training_set, window)\n",
        "\n",
        "    # Take the list of dictionaries describing the prediction errors on each window\n",
        "    # and turn it into a DataFrame\n",
        "    errors = pd.DataFrame(errors)\n",
        "\n",
        "    # Return the arithmetic mean of the prediction error across all windows.\n",
        "    # This is OK since our errors were already arithemtic means of some other values.\n",
        "    mean_error = errors.mean()\n",
        "\n",
        "    return predictions, errors, mean_error"
      ],
      "metadata": {
        "id": "FGnjqJN1_TmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(predictions):\n",
        "    complete_forecast = pd.concat(predictions).to_numpy().reshape(1, -1)\n",
        "    complete_forecast = scalers[TARGET_VARIABLE].inverse_transform(complete_forecast)\n",
        "\n",
        "    #offset = training_set.close.iloc[0]\n",
        "    #complete_forecast = reverse_differences(complete_forecast, offset)\n",
        "\n",
        "    expected_values = validation_set[TARGET_VARIABLE].to_numpy().reshape(1, -1)\n",
        "    expected_values = scalers[TARGET_VARIABLE].inverse_transform(expected_values)\n",
        "    #expected_values = reverse_differences(expected_values, offset)\n",
        "\n",
        "    x = validation_set.index\n",
        "    plt.plot(x, expected_values.reshape(-1))\n",
        "    plt.plot(x, complete_forecast.reshape(-1))"
      ],
      "metadata": {
        "id": "RbHLmWWpN4N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_errors(errors):\n",
        "    \"\"\"Plots the values attained by the error functions as the model\n",
        "    progressed through the validation dataset. \n",
        "    \n",
        "    `errors` must be a pandas `DataFrame` where the columns are the names of\n",
        "    the error functions, as defined in the `ERRORS` constant, and the rows are\n",
        "    the values of the corresponding error functions.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(nrows=len(ERRORS))\n",
        "\n",
        "    for index, error in enumerate(ERRORS):\n",
        "        ax[index].plot(errors[error], label=error)\n",
        "\n",
        "    return fig, ax"
      ],
      "metadata": {
        "id": "mkmt3uEmL_B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline model - constant prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "Hk_5PzUHvuAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a baseline, we'll simply take the values from the last day we have in the training set and predict that the stock market will stay constant after that ([the naÃ¯ve approach to forecasting](https://en.wikipedia.org/wiki/Forecasting#Na.C3.AFve_approach)):"
      ],
      "metadata": {
        "id": "qe6RtrKMtdTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstantStockPriceModel(StockPriceForecastingModel):\n",
        "    def train(self, data, delta):\n",
        "        self.last_closing_price = data[TARGET_VARIABLE].iloc[-1]\n",
        "\n",
        "    def predict(self, timestamps):\n",
        "        forecasted_closing_prices = np.tile(self.last_closing_price, timestamps.shape)\n",
        "        return pd.Series(index=timestamps, data=forecasted_closing_prices)"
      ],
      "metadata": {
        "id": "860zl1lItySZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConstantStockPriceModel()\n",
        "\n",
        "predictions, errors, mean_error = train_and_evaluate_model(model)"
      ],
      "metadata": {
        "id": "NgTj96j5JYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yrDt6cNEuOxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, _ = plot_errors(errors)\n",
        "fig.suptitle('Baseline/constant model errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A_LfLd5NMObO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error"
      ],
      "metadata": {
        "id": "ggZiWS3oSdX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common code for sklearn-based models"
      ],
      "metadata": {
        "id": "bHXVYRcphRL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SklearnModel(StockPriceForecastingModel):\n",
        "    def train(self, data, delta):\n",
        "        # Determine how many full windows of data we can generate from the given training data.\n",
        "        num_windows = len(data) // WINDOW_SIZE - 1\n",
        "\n",
        "        # Since the length of the training data might not be evenly divisible by the length of a window,\n",
        "        # we'll have to start from an offset.\n",
        "        offset = len(data) % WINDOW_SIZE\n",
        "\n",
        "        # Extract a subset of the data, to be used for creating windows of input/target variables.\n",
        "        available_training_data = data[AVAILABLE_VARIABLES].iloc[offset:offset + num_windows * WINDOW_SIZE].to_numpy()\n",
        "        available_target_data = data[TARGET_VARIABLE].iloc[offset + WINDOW_SIZE:offset + WINDOW_SIZE + num_windows * WINDOW_SIZE].to_numpy()\n",
        "\n",
        "        row_size = WINDOW_SIZE * len(AVAILABLE_VARIABLES)\n",
        "\n",
        "        X = available_training_data.reshape(num_windows, row_size)\n",
        "        y = available_target_data.reshape(num_windows, WINDOW_SIZE)\n",
        "\n",
        "        # Fit the model on the data.        \n",
        "        self.model.fit(X, y)\n",
        "\n",
        "        # The last window of available data will be used for prediction.\n",
        "        self.last_window = data[AVAILABLE_VARIABLES].iloc[-WINDOW_SIZE:].to_numpy().reshape(1, row_size)\n",
        "\n",
        "    def predict(self, timestamps):\n",
        "        forecasted_closing_prices = self.model.predict(self.last_window)\n",
        "        forecasted_closing_prices = forecasted_closing_prices[0]\n",
        "\n",
        "        # Since the validation dataset might not be broken up evenly into windows,\n",
        "        # we could be asked to forecast the closing stock price for a period of time\n",
        "        # shorter than 5 days.\n",
        "        forecasted_closing_prices = forecasted_closing_prices[:len(timestamps)]\n",
        "\n",
        "        return pd.Series(index=timestamps, data=forecasted_closing_prices)"
      ],
      "metadata": {
        "id": "hfmjS2YYhVHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear regression"
      ],
      "metadata": {
        "id": "KE6K6f3mP-bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another very simple kind of model is linear regression. It will take as input only the last five days of data to predict the next five days."
      ],
      "metadata": {
        "id": "PN_HPubJQCdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "class LinearRegressionModel(SklearnModel):\n",
        "    def __init__(self):\n",
        "        self.model = LinearRegression()"
      ],
      "metadata": {
        "id": "DoEe1qnoQE6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressionModel()\n",
        "\n",
        "predictions, errors, mean_error = train_and_evaluate_model(model)"
      ],
      "metadata": {
        "id": "CCoa6AI6TcFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CTpLHnRYWuhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, _ = plot_errors(errors)\n",
        "fig.suptitle('Linear regression model errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EVsBbD4-WxST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error"
      ],
      "metadata": {
        "id": "7hk15W8AWzGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random forest"
      ],
      "metadata": {
        "id": "PXiEfQqxi2L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "class RandomForestModel(SklearnModel):\n",
        "    def __init__(self):\n",
        "        self.model = RandomForestRegressor(max_samples=0.5)"
      ],
      "metadata": {
        "id": "T7X1M9uFi4vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestModel()\n",
        "\n",
        "predictions, errors, mean_error = train_and_evaluate_model(model)"
      ],
      "metadata": {
        "id": "lTxDRvNrjHzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6958n0UljKMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, _ = plot_errors(errors)\n",
        "fig.suptitle('Random forest model errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yES96Yr5jLt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error"
      ],
      "metadata": {
        "id": "E2IC3VUCjNQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient-boosted trees"
      ],
      "metadata": {
        "id": "negNQqXekZzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade xgboost>=1.6"
      ],
      "metadata": {
        "id": "AflHJCHnmsH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "class XGBRegressorModel(SklearnModel):\n",
        "    def __init__(self):\n",
        "        self.model = XGBRegressor(objective='reg:squarederror', subsample=0.25)"
      ],
      "metadata": {
        "id": "xIjX6kT5kcSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBRegressorModel()\n",
        "\n",
        "predictions, errors, mean_error = train_and_evaluate_model(model)"
      ],
      "metadata": {
        "id": "wl4526HCkxkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aCy0w7A_k3s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, _ = plot_errors(errors)\n",
        "fig.suptitle('XGB regressor model errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "41M_5p8Dk6ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error"
      ],
      "metadata": {
        "id": "zGBkA-1xk8jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output file"
      ],
      "metadata": {
        "id": "GwD2gwKrYqQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sheet_names = ('GS', 'C', 'WFC', 'BAC', 'JPM')\n",
        "dates = ('21-04-22', '22-04-22', '25-04-22', '26-04-22', '27-04-22')"
      ],
      "metadata": {
        "id": "wJ3X7jDuYs8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}